import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForMaskedLM
import pandas as pd
from sklearn.model_selection import train_test_split
from tqdm import tqdm  # åŠ è½½è¿›åº¦æ¡

# ========== é…ç½® ==========
MODEL_NAME = 'InstaDeepAI/agro-nucleotide-transformer-1b'
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BATCH_SIZE = 128  
NUM_EPOCHS = 100
LEARNING_RATE = 1e-3
MAX_SEQ_LENGTH = 512  # AgroNT æœ€å¤§æ”¯æŒé•¿åº¦
WINDOW_STRIDE = 256   # æ»‘åŠ¨çª—å£æ­¥é•¿

# ========== æ•°æ®é›†ç±» ==========
class ExpressionDataset(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = sequences
        self.labels = labels

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        seq = self.sequences[idx]
        label = self.labels[idx]
        return seq, label

# ========== MLP åˆ†ç±»å™¨ ==========
class MLPClassifier(nn.Module):
    def __init__(self, embedding_dim):
        super(MLPClassifier, self).__init__()
        self.classifier = nn.Sequential(
            nn.Linear(embedding_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 1),  # äºŒåˆ†ç±»
            nn.Sigmoid()
        )

    def forward(self, embeddings):
        return self.classifier(embeddings)

# ========== æå–åºåˆ—embedding ==========
def get_sequence_embedding(seq, tokenizer, model):
    """
    ä½¿ç”¨ AgroNT ç¼–ç åºåˆ—ï¼Œæ”¯æŒæ»‘åŠ¨çª—å£
    """
    embeddings = []
    for i in range(0, len(seq), WINDOW_STRIDE):
        subseq = seq[i:i+MAX_SEQ_LENGTH]
        tokens = tokenizer(subseq,
                           return_tensors="pt",
                           padding="max_length",
                           truncation=True,
                           max_length=MAX_SEQ_LENGTH).to(DEVICE)
        with torch.no_grad():
            output = model(**tokens, output_hidden_states=True)
            hidden_state = output.hidden_states[-1]  # æœ€åä¸€å±‚ hidden state
            pooled = hidden_state.mean(dim=1)  
    return torch.stack(embeddings).mean(dim=0)

# ========== åŠ è½½æ•°æ® ==========
def load_data(csv_path, test_size=0.2):
    df = pd.read_csv(csv_path)
    sequences = df['sequence'].tolist()
    labels = df['label'].tolist()
    return train_test_split(sequences, labels, test_size=test_size, random_state=42, stratify=labels)

# ========== è®­ç»ƒå‡½æ•° ==========
def train_model(model, classifier, train_loader, optimizer, criterion, tokenizer):
    model.eval()  # AgroNT å†»ç»“
    classifier.train()
    total_loss = 0

    # å¤–å±‚è¿›åº¦æ¡ï¼šæ•´ä¸ªè®­ç»ƒé›†
    for sequences, labels in tqdm(train_loader, desc="Training"):
        labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1).to(DEVICE)

        embeddings_batch = []
        # å†…å±‚è¿›åº¦æ¡ï¼šå½“å‰ batch å†…æ¯æ¡åºåˆ—
        for seq in tqdm(sequences, desc="Embedding Seq", leave=False):
            emb = get_sequence_embedding(seq, tokenizer, model)
            embeddings_batch.append(emb)
        embeddings_batch = torch.cat(embeddings_batch, dim=0).to(DEVICE)

        # åˆ†ç±»é¢„æµ‹
        preds = classifier(embeddings_batch)
        loss = criterion(preds, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    return avg_loss

# ========== éªŒè¯å‡½æ•° ==========
def evaluate_model(model, classifier, val_loader, tokenizer):
    model.eval()
    classifier.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for sequences, labels in tqdm(val_loader, desc="Evaluating"):
            labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1).to(DEVICE)

            embeddings_batch = []
            for seq in tqdm(sequences, desc="Embedding Seq (Val)", leave=False):
                emb = get_sequence_embedding(seq, tokenizer, model)
                embeddings_batch.append(emb)
            embeddings_batch = torch.cat(embeddings_batch, dim=0).to(DEVICE)

            preds = classifier(embeddings_batch)
            predicted = (preds > 0.5).float()
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

    acc = correct / total * 100
    return acc

# ========== ä¸»å‡½æ•° ==========
def main(csv_path):
    print("ğŸš€ åŠ è½½ tokenizer å’Œ AgroNT æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    agro_nt = AutoModelForMaskedLM.from_pretrained(MODEL_NAME).to(DEVICE)
    for param in agro_nt.parameters():
        param.requires_grad = False  # å†»ç»“ AgroNT æƒé‡

    classifier = MLPClassifier(embedding_dim=agro_nt.config.hidden_size).to(DEVICE)
    optimizer = optim.Adam(classifier.parameters(), lr=LEARNING_RATE)
    criterion = nn.BCELoss()

    print("ğŸ“„ åŠ è½½æ•°æ®é›†...")
    train_seqs, val_seqs, train_labels, val_labels = load_data(csv_path)
    train_dataset = ExpressionDataset(train_seqs, train_labels)
    val_dataset = ExpressionDataset(val_seqs, val_labels)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

    print("ğŸ‹ï¸â€â™‚ï¸ å¼€å§‹è®­ç»ƒ...")
    for epoch in range(NUM_EPOCHS):
        train_loss = train_model(agro_nt, classifier, train_loader, optimizer, criterion, tokenizer)
        val_acc = evaluate_model(agro_nt, classifier, val_loader, tokenizer)
        print(f"Epoch [{epoch+1}/{NUM_EPOCHS}] - Train Loss: {train_loss:.4f} - Val Accuracy: {val_acc:.2f}%")

    torch.save(classifier.state_dict(), "mlp_classifier.pth")
    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œåˆ†ç±»å™¨å·²ä¿å­˜ä¸º mlp_classifier.pth")

# ========== è¿è¡Œ ==========
if __name__ == "__main__":
    csv_file = "a/output_sequences_target.csv"  # CSVæ–‡ä»¶è·¯å¾„
    main(csv_file)

