import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForMaskedLM
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import roc_auc_score  # ğŸ†• è®¡ç®— AUROC
import os
from datetime import datetime

# ========== é…ç½® ==========
MODEL_NAME = 'InstaDeepAI/agro-nucleotide-transformer-1b'
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BATCH_SIZE = 128  # å¦‚æœæ˜¾å­˜ä¸è¶³å¯è°ƒå°
NUM_EPOCHS = 3
LEARNING_RATE = 1e-3
MAX_SEQ_LENGTH = 512  # AgroNT æœ€å¤§æ”¯æŒé•¿åº¦
WINDOW_STRIDE = 256   # æ»‘åŠ¨çª—å£æ­¥é•¿

# ========== æ•°æ®é›†ç±» ==========
class ExpressionDataset(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = sequences
        self.labels = labels

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        seq = self.sequences[idx]
        label = self.labels[idx]
        return seq, label

# ========== MLP åˆ†ç±»å™¨ ==========
class MLPClassifier(nn.Module):
    def __init__(self, embedding_dim):
        super(MLPClassifier, self).__init__()
        self.classifier = nn.Sequential(
            nn.Linear(embedding_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 1),  # äºŒåˆ†ç±»
            nn.Sigmoid()
        )

    def forward(self, embeddings):
        return self.classifier(embeddings)

# ========== æå–åºåˆ—embedding ==========
def get_sequence_embedding(seq, tokenizer, model):
    """
    ä½¿ç”¨ AgroNT ç¼–ç åºåˆ—ï¼Œæ”¯æŒæ»‘åŠ¨çª—å£
    """
    embeddings = []
    for i in range(0, len(seq), WINDOW_STRIDE):
        subseq = seq[i:i+MAX_SEQ_LENGTH]
        tokens = tokenizer(subseq,
                           return_tensors="pt",
                           padding="max_length",
                           truncation=True,
                           max_length=MAX_SEQ_LENGTH).to(DEVICE)
        with torch.no_grad():
            output = model(**tokens, output_hidden_states=True)
            hidden_state = output.hidden_states[-1]  # æœ€åä¸€å±‚ hidden state
            pooled = hidden_state.mean(dim=1)  # mean pooling
            embeddings.append(pooled)
    return torch.stack(embeddings).mean(dim=0)

# ========== æŒ‰æŸ“è‰²ä½“åˆ’åˆ†æ•°æ® ==========
def load_data_by_chrom(df, leave_out_chrom):
    """
    - è®­ç»ƒé›†ï¼šé™¤leave_out_chromå¤–çš„æ‰€æœ‰æ•°æ®
    - éªŒè¯é›†ï¼šä»…leave_out_chromçš„æ•°æ®
    """
    train_df = df[df['chrom'] != leave_out_chrom]
    val_df = df[df['chrom'] == leave_out_chrom]

    train_sequences = train_df['full_sequence'].tolist()
    train_labels = train_df['target'].tolist()

    val_sequences = val_df['full_sequence'].tolist()
    val_labels = val_df['target'].tolist()

    return train_sequences, train_labels, val_sequences, val_labels

# ========== è®­ç»ƒå‡½æ•° ==========
def train_model(model, classifier, train_loader, optimizer, criterion, tokenizer):
    model.eval()  # AgroNT å†»ç»“
    classifier.train()
    total_loss = 0

    for sequences, labels in tqdm(train_loader, desc="Training"):
        labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1).to(DEVICE)

        embeddings_batch = []
        for seq in tqdm(sequences, desc="Embedding Seq", leave=False):
            emb = get_sequence_embedding(seq, tokenizer, model)
            embeddings_batch.append(emb)
        embeddings_batch = torch.cat(embeddings_batch, dim=0).to(DEVICE)

        preds = classifier(embeddings_batch)
        loss = criterion(preds, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    return avg_loss

# ========== éªŒè¯å‡½æ•° ==========
def evaluate_model(model, classifier, val_loader, tokenizer):
    model.eval()
    classifier.eval()
    correct, total = 0, 0
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for sequences, labels in tqdm(val_loader, desc="Evaluating"):
            labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1).to(DEVICE)

            embeddings_batch = []
            for seq in tqdm(sequences, desc="Embedding Seq (Val)", leave=False):
                emb = get_sequence_embedding(seq, tokenizer, model)
                embeddings_batch.append(emb)
            embeddings_batch = torch.cat(embeddings_batch, dim=0).to(DEVICE)

            probs = classifier(embeddings_batch)
            predicted = (probs > 0.5).float()

            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

            correct += (predicted == labels).sum().item()
            total += labels.size(0)

    acc = correct / total * 100

    # ğŸ†• è®¡ç®— AUROC
    try:
        auroc = roc_auc_score(all_labels, all_probs) * 100
    except ValueError:
        # å¦‚æœåªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œä¼šæŠ¥é”™
        auroc = float('nan')

    return acc, auroc

# ========== ä¸»å‡½æ•° ==========
def main(csv_path, checkpoint_dir="./checkpoints"):
    print("ğŸš€ åŠ è½½ tokenizer å’Œ AgroNT æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    agro_nt = AutoModelForMaskedLM.from_pretrained(MODEL_NAME).to(DEVICE)
    for param in agro_nt.parameters():
        param.requires_grad = False  # å†»ç»“ AgroNT æƒé‡

    # ç¡®ä¿ checkpoint ç›®å½•å­˜åœ¨
    os.makedirs(checkpoint_dir, exist_ok=True)

    # è¯»å–å®Œæ•´æ•°æ®
    df = pd.read_csv(csv_path)

    # å¾ªç¯æ¯ä¸ªæŸ“è‰²ä½“
    for leave_out_chrom in sorted(df['chrom'].unique()):
        print(f"\nğŸŒ± å½“å‰è½®æ¬¡: ç•™å‡ºæŸ“è‰²ä½“ {leave_out_chrom} ä½œä¸ºéªŒè¯é›†")
        train_seqs, train_labels, val_seqs, val_labels = load_data_by_chrom(df, leave_out_chrom)

        train_dataset = ExpressionDataset(train_seqs, train_labels)
        val_dataset = ExpressionDataset(val_seqs, val_labels)
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

        classifier = MLPClassifier(embedding_dim=agro_nt.config.hidden_size).to(DEVICE)
        optimizer = optim.Adam(classifier.parameters(), lr=LEARNING_RATE)
        criterion = nn.BCELoss()

        # æ—¥å¿—æ–‡ä»¶ & æ¨¡å‹æ–‡ä»¶
        log_file = os.path.join(checkpoint_dir, f"log_chrom{leave_out_chrom}.txt")
        checkpoint = os.path.join(checkpoint_dir, f"mlp_classifier_chrom{leave_out_chrom}.pth")

        with open(log_file, "w") as f:
            for epoch in range(NUM_EPOCHS):
                print(f"ğŸ‹ï¸â€â™‚ï¸ Epoch [{epoch+1}/{NUM_EPOCHS}] for Chrom {leave_out_chrom}")
                train_loss = train_model(agro_nt, classifier, train_loader, optimizer, criterion, tokenizer)
                val_acc, val_auroc = evaluate_model(agro_nt, classifier, val_loader, tokenizer)

                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                log_str = (f"[{timestamp}] Chrom {leave_out_chrom} - Epoch [{epoch+1}/{NUM_EPOCHS}] "
                           f"- Train Loss: {train_loss:.4f} - Val Accuracy: {val_acc:.2f}% - Val AUROC: {val_auroc:.2f}%")
                print(log_str)
                f.write(log_str + "\n")

                # ä¿å­˜æ–­ç‚¹
                torch.save(classifier.state_dict(), checkpoint)

        print(f"âœ… æŸ“è‰²ä½“ {leave_out_chrom} æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œåˆ†ç±»å™¨å·²ä¿å­˜ä¸º {checkpoint}")

    print("ğŸ‰ æ‰€æœ‰10è½®è®­ç»ƒå®Œæˆ")

# ========== è¿è¡Œ ==========
if __name__ == "__main__":
    csv_file = "/home/miaoshiqi/a/zea_root_seq_target.csv"  # CSVè·¯å¾„
    main(csv_file, checkpoint_dir="/home/miaoshiqi/a/checkpoints")
